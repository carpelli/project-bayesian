{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63456dea",
   "metadata": {},
   "source": [
    "# Frequentist Naive Bayes Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2524973",
   "metadata": {},
   "source": [
    "In this notebook we are going to study a Frequentist approach of the Naive Bayes Classifier, to do so, we define a class that given an aribitrary dataset, and a probability distribution, calculate the Naive Bayes Classifier for that given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "759fd7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f5796",
   "metadata": {},
   "source": [
    "## Class for Naive Bayes Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "865f4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "  # all probabilities inside the class are log\n",
    "\n",
    "  def __init__(self, *, param_func, prob_func, param_priors=None):\n",
    "    # user-provided function to compute the distribution parameters\n",
    "    # the function should return an list of parameter arrays (...)\n",
    "    self.param_func = param_func\n",
    "\n",
    "    # user-provided function to compute the probabilities from the parameters\n",
    "    # the input are the parameter arrays and the X\n",
    "    self.prob_func = prob_func\n",
    "\n",
    "    # hyperparameters of the parameter priors used by the prob_func in case a\n",
    "    # Bayesian approach is taken\n",
    "    self.param_priors = [param_priors] if param_priors is not None else []\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    # retrieve the number of samples and features from X\n",
    "    self.n_samples, self.n_features = X.shape\n",
    "\n",
    "    # collect the individual classes, and record their counts to compute priors\n",
    "    self.classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    # compute the parameters for every class - we transpose the array so that\n",
    "    # the final dimensions are (parameters, classes, features)\n",
    "    self.params = np.array(\n",
    "      [self.param_func(X[c==y], *self.param_priors) for c in self.classes]\n",
    "    ).transpose(1, 0, 2)\n",
    "\n",
    "    # compute the priors from the counts\n",
    "    self.class_probs = np.log(counts/self.n_samples)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def posteriors(self, X):\n",
    "    # reshape X to fit the array dimensions (samples, classes, features)\n",
    "    X = np.reshape(X, (-1, 1, self.n_features))\n",
    "\n",
    "    # compute the probabilities of the samples (...)\n",
    "    probs = np.log(self.prob_func(X, *self.params[:,np.newaxis]))\n",
    "    return probs.sum(axis=2) + self.class_probs[np.newaxis]\n",
    "\n",
    "  def predict_proba(self, X):\n",
    "    # predict the probabilities for each class\n",
    "    exp_post = np.exp(self.posteriors(X))\n",
    "    return exp_post / exp_post.sum()\n",
    "\n",
    "  def predict(self, X):\n",
    "    # predict the class with the maximum probability\n",
    "    return self.classes[np.argmax(self.posteriors(X), axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52496ac",
   "metadata": {},
   "source": [
    "# Checking mail spam:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b23e6a",
   "metadata": {},
   "source": [
    "Given an e-mail message, determine wheather such message is Spam or Ham, text classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fcfe4beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/spam.csv\", encoding= 'latin-1')\n",
    "data = data[[\"class\", \"message\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6de41fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data[\"message\"])\n",
    "y = np.array(data[\"class\"])\n",
    "\n",
    "cv = CountVectorizer() # count the amout of occurrences for each word\n",
    "x = cv.fit_transform(x)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b376786",
   "metadata": {},
   "source": [
    "Note that we have the value counts with valus up to 18 occurrences, we can work then with Bernoulli, Multinomial and Gaussian Naive Bayes, althought it is clear that most likely multinomial will be the most efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cf8ceb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 12, 15, 18],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_dense = xtrain.toarray()#number of distinct diferent occurrences!\n",
    "np.unique(no_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2bb61",
   "metadata": {},
   "source": [
    "### Using Bernoulli Naive Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab861ca",
   "metadata": {},
   "source": [
    "**Must check if it's neccesary to prepare the data to 0 and 1 inputs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2c2e56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the class with to the proper functions and fit to the data\n",
    "nb = NaiveBayes(\n",
    "  param_func=lambda X: [X.mean(axis=0)],         # return probability of p(x_{k,c} = 1)\n",
    "  prob_func=lambda x, phi: phi*x + (1-phi)*(1-x) # Bernoulli pmf\n",
    ").fit(xtrain.toarray(), ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9d6a107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marct\\AppData\\Local\\Temp\\ipykernel_14688\\3204703672.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "  probs = np.log(self.prob_func(X, *self.params[:,np.newaxis]))\n",
      "C:\\Users\\marct\\AppData\\Local\\Temp\\ipykernel_14688\\3204703672.py:40: RuntimeWarning: invalid value encountered in log\n",
      "  probs = np.log(self.prob_func(X, *self.params[:,np.newaxis]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.915189589409917\n",
      "The testing error is: 0.8860986547085202\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = nb.predict(xtest.toarray())\n",
    "y_pred_train = nb.predict(xtrain.toarray())\n",
    "print(f\"The training error is: {(y_pred_train == ytrain).mean()}\")\n",
    "print(f\"The testing error is: {(y_pred_test == ytest).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c2533",
   "metadata": {},
   "source": [
    "### Using Multinomial Naive Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e4ab4",
   "metadata": {},
   "source": [
    "**Must finish and define the multinomial probability distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd4b6d",
   "metadata": {},
   "source": [
    "### Using Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f3e04f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_var(X):\n",
    "  return [X.mean(axis=0), X.var(axis=0)]\n",
    "def gauss(x, mean, var):\n",
    "  return np.exp(-(x-mean)**2/(2*var)) / np.sqrt(var*2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cc61f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(x.toarray()) #to standarize features to a zero mean and 1 variance!\n",
    "xtrain = scaler.transform(xtrain.toarray())\n",
    "xtest = scaler.transform(xtest.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739cc5c2",
   "metadata": {},
   "source": [
    "Let's now fit the model with the parameters scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f0ca1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(param_func=mean_var, prob_func=gauss).fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1b8d25",
   "metadata": {},
   "source": [
    "And finally compute the accuracy in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "dcbfc5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marct\\AppData\\Local\\Temp\\ipykernel_14688\\3627416108.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.exp(-(x-mean)**2/(2*var)) / np.sqrt(var*2*np.pi)\n",
      "C:\\Users\\marct\\AppData\\Local\\Temp\\ipykernel_14688\\3627416108.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(-(x-mean)**2/(2*var)) / np.sqrt(var*2*np.pi)\n",
      "C:\\Users\\marct\\AppData\\Local\\Temp\\ipykernel_14688\\3204703672.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "  probs = np.log(self.prob_func(X, *self.params[:,np.newaxis]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.8660533991474085\n",
      "The testing error is: 0.8654708520179372\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = nb.predict(xtest)\n",
    "y_pred_train = nb.predict(xtrain)\n",
    "print(f\"The training error is: {(y_pred_train == ytrain).mean()}\")\n",
    "print(f\"The testing error is: {(y_pred_test == ytest).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cdfa5",
   "metadata": {},
   "source": [
    "# Raisin dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d0941",
   "metadata": {},
   "source": [
    "The raisin dataset was from a study of machine vision systems in order to distinguish between two distinct varieties of raisins, Kecimen and Besni, a total of 900 pices of raisin were obtained and distinct metric values were recorded and included in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d6adc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Raisin_Dataset.csv')\n",
    "x = np.array(df.drop(\"Class\", axis=1))\n",
    "y = np.array(df[\"Class\"])\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "eb249f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87524</td>\n",
       "      <td>442.246011</td>\n",
       "      <td>253.291155</td>\n",
       "      <td>0.819738</td>\n",
       "      <td>90546</td>\n",
       "      <td>0.758651</td>\n",
       "      <td>1184.040</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75166</td>\n",
       "      <td>406.690687</td>\n",
       "      <td>243.032436</td>\n",
       "      <td>0.801805</td>\n",
       "      <td>78789</td>\n",
       "      <td>0.684130</td>\n",
       "      <td>1121.786</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90856</td>\n",
       "      <td>442.267048</td>\n",
       "      <td>266.328318</td>\n",
       "      <td>0.798354</td>\n",
       "      <td>93717</td>\n",
       "      <td>0.637613</td>\n",
       "      <td>1208.575</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45928</td>\n",
       "      <td>286.540559</td>\n",
       "      <td>208.760042</td>\n",
       "      <td>0.684989</td>\n",
       "      <td>47336</td>\n",
       "      <td>0.699599</td>\n",
       "      <td>844.162</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79408</td>\n",
       "      <td>352.190770</td>\n",
       "      <td>290.827533</td>\n",
       "      <td>0.564011</td>\n",
       "      <td>81463</td>\n",
       "      <td>0.792772</td>\n",
       "      <td>1073.251</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>83248</td>\n",
       "      <td>430.077308</td>\n",
       "      <td>247.838695</td>\n",
       "      <td>0.817263</td>\n",
       "      <td>85839</td>\n",
       "      <td>0.668793</td>\n",
       "      <td>1129.072</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>87350</td>\n",
       "      <td>440.735698</td>\n",
       "      <td>259.293149</td>\n",
       "      <td>0.808629</td>\n",
       "      <td>90899</td>\n",
       "      <td>0.636476</td>\n",
       "      <td>1214.252</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>99657</td>\n",
       "      <td>431.706981</td>\n",
       "      <td>298.837323</td>\n",
       "      <td>0.721684</td>\n",
       "      <td>106264</td>\n",
       "      <td>0.741099</td>\n",
       "      <td>1292.828</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>93523</td>\n",
       "      <td>476.344094</td>\n",
       "      <td>254.176054</td>\n",
       "      <td>0.845739</td>\n",
       "      <td>97653</td>\n",
       "      <td>0.658798</td>\n",
       "      <td>1258.548</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>85609</td>\n",
       "      <td>512.081774</td>\n",
       "      <td>215.271976</td>\n",
       "      <td>0.907345</td>\n",
       "      <td>89197</td>\n",
       "      <td>0.632020</td>\n",
       "      <td>1272.862</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Area  MajorAxisLength  MinorAxisLength  Eccentricity  ConvexArea  \\\n",
       "0    87524       442.246011       253.291155      0.819738       90546   \n",
       "1    75166       406.690687       243.032436      0.801805       78789   \n",
       "2    90856       442.267048       266.328318      0.798354       93717   \n",
       "3    45928       286.540559       208.760042      0.684989       47336   \n",
       "4    79408       352.190770       290.827533      0.564011       81463   \n",
       "..     ...              ...              ...           ...         ...   \n",
       "895  83248       430.077308       247.838695      0.817263       85839   \n",
       "896  87350       440.735698       259.293149      0.808629       90899   \n",
       "897  99657       431.706981       298.837323      0.721684      106264   \n",
       "898  93523       476.344094       254.176054      0.845739       97653   \n",
       "899  85609       512.081774       215.271976      0.907345       89197   \n",
       "\n",
       "       Extent  Perimeter    Class  \n",
       "0    0.758651   1184.040  Kecimen  \n",
       "1    0.684130   1121.786  Kecimen  \n",
       "2    0.637613   1208.575  Kecimen  \n",
       "3    0.699599    844.162  Kecimen  \n",
       "4    0.792772   1073.251  Kecimen  \n",
       "..        ...        ...      ...  \n",
       "895  0.668793   1129.072    Besni  \n",
       "896  0.636476   1214.252    Besni  \n",
       "897  0.741099   1292.828    Besni  \n",
       "898  0.658798   1258.548    Besni  \n",
       "899  0.632020   1272.862    Besni  \n",
       "\n",
       "[900 rows x 8 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef4747",
   "metadata": {},
   "source": [
    "Naturally given that the data presented is continous, of the studied models we can only apply the Gaussian Naive Bayes. Let's observe the yielding results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d80f317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_var(X):\n",
    "  return [X.mean(axis=0), X.var(axis=0)]\n",
    "def gauss(x, mean, var):\n",
    "  return np.exp(-(x-mean)**2/(2*var)) / np.sqrt(var*2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e60193c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(xtrain) #to standarize features to a zero mean and 1 variance!\n",
    "xtrain = scaler.transform(xtrain)\n",
    "xtest = scaler.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfdb200",
   "metadata": {},
   "source": [
    "Let's now fit the model with the parameters scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "68442ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(param_func=mean_var, prob_func=gauss).fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ba589",
   "metadata": {},
   "source": [
    "And finally compute the accuracy in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "faba715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.8375\n",
      "The testing error is: 0.8388888888888889\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = nb.predict(xtest)\n",
    "y_pred_train = nb.predict(xtrain)\n",
    "print(f\"The training error is: {(y_pred_train == ytrain).mean()}\")\n",
    "print(f\"The testing error is: {(y_pred_test == ytest).mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad0b84",
   "metadata": {},
   "source": [
    "## IMDB Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7373c837",
   "metadata": {},
   "source": [
    "**Can't work with a dataset so big without sparse matrices, and hence, if the class is unable to do so, we can avoid using this dataset!!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79ccc04",
   "metadata": {},
   "source": [
    "**How to proceed is given just in case is usefull at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b2b3b",
   "metadata": {},
   "source": [
    "Given a review from a film in IMDB, analyze it's sentiment and classify between positive and negative review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e8a69949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/IMDB_Dataset.csv')\n",
    "#https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1fcac5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "149766cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [166]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer() \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m xtrain, xtest, ytrain, ytest \u001b[38;5;241m=\u001b[39m train_test_split(x, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TFM\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1207\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1205\u001b[0m         feature_counter[feature_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1207\u001b[0m         feature_counter[feature_idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;66;03m# Ignore out-of-vocabulary items for fixed_vocab=True\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = np.array(df[\"review\"])\n",
    "y = np.array(df[\"sentiment\"])\n",
    "cv = CountVectorizer() #\n",
    "x = cv.fit_transform(x)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df569f",
   "metadata": {},
   "source": [
    "Note that again, as in the Spam or Ham dataset, we have data with multiplicites, we expect the Multinomial Naive Bayes to perform the best, but we can actually perform tests in the 3 distinct models presented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052b5ec",
   "metadata": {},
   "source": [
    "### Using Bernoulli Naive Bayes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7523f045",
   "metadata": {},
   "source": [
    "**Must check if it's neccesary to prepare the data to 0 and 1 inputs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af353b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the class with to the proper functions and fit to the data\n",
    "nb = NaiveBayes(\n",
    "  param_func=lambda X: [X.mean(axis=0)],         # return probability of p(x_{k,c} = 1)\n",
    "  prob_func=lambda x, phi: phi*x + (1-phi)*(1-x) # Bernoulli pmf\n",
    ").fit(xtrain.toarray(), ytrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d8066",
   "metadata": {},
   "source": [
    "## Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "40c22697",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feab817",
   "metadata": {},
   "source": [
    "This dataset is similar to the Raisin dataset, this set of data containd 50 distinct samples of 3 distinct Iris plants, Iris setosa, Iris viginica and Iris Versicolor. 4  distinct features were recorded for each plant, we would want to be able to tell apart each of this type of Iris just with this given 4 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "281366fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>146</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>147</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>148</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>149</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "0      1            5.1           3.5            1.4           0.2   \n",
       "1      2            4.9           3.0            1.4           0.2   \n",
       "2      3            4.7           3.2            1.3           0.2   \n",
       "3      4            4.6           3.1            1.5           0.2   \n",
       "4      5            5.0           3.6            1.4           0.2   \n",
       "..   ...            ...           ...            ...           ...   \n",
       "145  146            6.7           3.0            5.2           2.3   \n",
       "146  147            6.3           2.5            5.0           1.9   \n",
       "147  148            6.5           3.0            5.2           2.0   \n",
       "148  149            6.2           3.4            5.4           2.3   \n",
       "149  150            5.9           3.0            5.1           1.8   \n",
       "\n",
       "            Species  \n",
       "0       Iris-setosa  \n",
       "1       Iris-setosa  \n",
       "2       Iris-setosa  \n",
       "3       Iris-setosa  \n",
       "4       Iris-setosa  \n",
       "..              ...  \n",
       "145  Iris-virginica  \n",
       "146  Iris-virginica  \n",
       "147  Iris-virginica  \n",
       "148  Iris-virginica  \n",
       "149  Iris-virginica  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414bca4",
   "metadata": {},
   "source": [
    "As in the Raisin Dataset, data is continous, and hence, the only course of action would be to approach it using a Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2936167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df.drop(\"Species\", axis=1))\n",
    "y = np.array(df[\"Species\"])\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c107750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_var(X):\n",
    "  return [X.mean(axis=0), X.var(axis=0)]\n",
    "def gauss(x, mean, var):\n",
    "  return np.exp(-(x-mean)**2/(2*var)) / np.sqrt(var*2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "bf3be975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(xtrain) #to standarize features to a zero mean and 1 variance!\n",
    "xtrain = scaler.transform(xtrain)\n",
    "xtest = scaler.transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ee013",
   "metadata": {},
   "source": [
    "Let's now fit the model with the parameters scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5c744f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(param_func=mean_var, prob_func=gauss).fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59085b8",
   "metadata": {},
   "source": [
    "And finally compute the accuracy in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "34c19c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training error is: 0.9916666666666667\n",
      "The testing error is: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = nb.predict(xtest)\n",
    "y_pred_train = nb.predict(xtrain)\n",
    "print(f\"The training error is: {(y_pred_train == ytrain).mean()}\")\n",
    "print(f\"The testing error is: {(y_pred_test == ytest).mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
