{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63456dea",
   "metadata": {},
   "source": [
    "# Bayesian Naive Bayes\n",
    "\n",
    "### Spring 2022 - Course Project\n",
    "\n",
    "### Authors: Marc Taberner, Otis Carpay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2524973",
   "metadata": {},
   "source": [
    "In this notebook we will first study the standard frequentist approach to the Naive Bayes Classifier. To do so, we define a base class for the classifier and subclass it for the Bernoulli, Multinomial, and Gaussian models. We test these classifiers on two text classification datasets and two continuous numerical datasets.\n",
    "\n",
    "The second part of the notebook treats a Bayesian approach. We will compare the frequentist approach with analytical Bayesian implementations of the Bernoulli and the Multinomial event model, and Stan implementation of all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a91dad",
   "metadata": {},
   "source": [
    "## Frequentist approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "24b71a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import multinomial, norm, bernoulli\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f5796",
   "metadata": {},
   "source": [
    "### Class for Naive Bayes Classifier:\n",
    "\n",
    "Here we define the base class, which will be subclassed for the specific event models. These event models will implement a `compute_params` function to compute the MLE estimators for the parameters (sufficient statistics) of the distributions, and a `compute_probabilities` function to compute the probabilities $P(X_{j} = x_{j}|C = i)$ for a given subset of the data of class $i$.\n",
    "\n",
    "The `fit` function calls the `compute_parameters` for the subset of data for every class and computes the class_probabilites $P(C = i)$.\n",
    "The function `posteriors` (following the term classically used for Naive Bayes and not referring to a Bayesian concept in this case) calculates\n",
    "$$g_{i}(x) = \\prod_{j= 1}^{n}P(X_{j} = x_{j}|C = i) P(C = i)$$\n",
    " and `predict` calculates \n",
    "$$\\argmax g_{i}(x).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "865f4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "  def compute_params(self, X):\n",
    "    # to be defined in a subclass\n",
    "    # function to compute the distribution parameters\n",
    "    # should return an list of parameter arrays of shape (classes, features)\n",
    "    return [[]]\n",
    "\n",
    "  def compute_probabilities(self, X, params):\n",
    "    # to be defined in a subclass\n",
    "    # function to compute the probabilities from the parameters\n",
    "    pass\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    # retrieve the number of samples and features from X\n",
    "    self.n_samples, self.n_features = X.shape\n",
    "\n",
    "    # collect the individual classes, and record their counts to compute priors\n",
    "    self.classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    # compute the parameters for every class - we transpose the array so that\n",
    "    # the final dimensions are (parameters, classes, features)\n",
    "    self.params = np.array(\n",
    "      [self.compute_params(X[c==y]) for c in self.classes]\n",
    "    ).transpose(1, 0, 2)\n",
    "\n",
    "    # compute the class probabilities from the counts\n",
    "    self.class_probs = np.log(counts/self.n_samples)\n",
    "\n",
    "    return self\n",
    "\n",
    "  def posteriors(self, X):\n",
    "    # reshape X to fit the array dimensions (samples, classes, features)\n",
    "    X = np.reshape(X, (-1, 1, self.n_features))\n",
    "\n",
    "    # compute the probabilities P(X_j = x_j | C = i)\n",
    "    probs = self.compute_probabilities(X, *self.params[:,np.newaxis])\n",
    "\n",
    "    # finally compute the posteriors by summing all log probs\n",
    "    return probs.sum(axis=2) + self.class_probs[np.newaxis]\n",
    "\n",
    "  def predict(self, X):\n",
    "    # predict the class with the maximum probability\n",
    "    return self.classes[np.argmax(self.posteriors(X), axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6cb3e",
   "metadata": {},
   "source": [
    "#### Bernoulli model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7c8d9",
   "metadata": {},
   "source": [
    "First we define the Bernoulli model. We include a smoothing parameters $\\alpha$ and set it to a small $10^{-10}$ by default to avoid computational problems. We clip the data to a maximum of 1 so we can apply the classifier to a multinomial dataset without any extra work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a6db3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB(NaiveBayes):\n",
    "  def __init__(self, alpha=1e-10):\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def compute_params(self, X):\n",
    "    # the MLE estimations are easily calculated by taking the mean\n",
    "    return [X.clip(max=1).mean(axis=0) + self.alpha]\n",
    "  \n",
    "  def compute_probabilities(self, X, phi):\n",
    "    return bernoulli.logpmf(X.clip(max=1), phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c55c9",
   "metadata": {},
   "source": [
    "#### Multinomial model\n",
    "\n",
    "Like the Bernoulli model, we include a smoothing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8147034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB(NaiveBayes):\n",
    "  def __init__(self, alpha=1e-10):\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def compute_params(self, X):\n",
    "    counts = X.sum(axis=0) + self.alpha\n",
    "    return [counts/counts.sum()]\n",
    "\n",
    "  def compute_probabilities(self, X, phi):\n",
    "    # we need to add a new axis to conform to the shape expected by\n",
    "    # the posteriors function: the feature axis is of size 1 since\n",
    "    # the distribution gives the probability of a whole sample\n",
    "    return multinomial.logpmf(X, X.sum(axis=2), phi)[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13afca",
   "metadata": {},
   "source": [
    "### Gaussian model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5337b169",
   "metadata": {},
   "source": [
    "Finally, the Gaussian model perhaps has the most straightforward code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "248616a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB(NaiveBayes):\n",
    "  def compute_params(self, X):\n",
    "    return [X.mean(axis=0), X.std(axis=0)]\n",
    "\n",
    "  def compute_probabilities(self, X, mean, sd):\n",
    "    return norm.logpdf(X, mean, sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52496ac",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c56e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clf(clf, xtrain, xtest, ytrain, ytest):\n",
    "  clf.fit(xtrain, ytrain)\n",
    "  y_pred_test = clf.predict(xtest)\n",
    "  y_pred_train = clf.predict(xtrain)\n",
    "  print(f\"The train error is: {(y_pred_train == ytrain).mean():.3f}\")\n",
    "  print(f\"The test error is: {(y_pred_test == ytest).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea243f4",
   "metadata": {},
   "source": [
    "### Spam classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b23e6a",
   "metadata": {},
   "source": [
    "Given an e-mail message, determine wheather such message is Spam or Ham, text classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "fcfe4beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/spam.csv\", encoding='latin-1')\n",
    "data = data[[\"class\", \"message\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e2ea3",
   "metadata": {},
   "source": [
    "We transform the documents to multinomial points with the sklearn `CountVectorizer` class. It transforms the data to vectors with the number of occurrences for each word. We set it to excludes words from a list of common words called 'stop words', and limit the words to the 1000 top words for computational speed. In our tests, the effect on accuracy was negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6de41fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data[\"message\"])\n",
    "y = np.array(data[\"class\"])\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', max_features=1000)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "\n",
    "xtrain = cv.fit_transform(xtrain).toarray()\n",
    "xtest = cv.transform(xtest).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b376786",
   "metadata": {},
   "source": [
    "Note that we have the value counts with valus up to 18 occurrences, we can work then with Bernoulli, Multinomial and Gaussian Naive Bayes, althought it is clear that most likely multinomial will be the most efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "cf8ceb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  8, 10, 15, 18])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2bb61",
   "metadata": {},
   "source": [
    "#### Using Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8e6547d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.990\n",
      "The test error is: 0.984\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(BernoulliNB(alpha=1e-4), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c2533",
   "metadata": {},
   "source": [
    "#### Using Multinomial Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "57a20d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.985\n",
      "The test error is: 0.983\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(MultinomialNB(alpha=2), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd4b6d",
   "metadata": {},
   "source": [
    "### Using Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "cc61f050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/otis/Documents/uni/2/Bayes/Project/venv/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:1945: RuntimeWarning: divide by zero encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "/Users/otis/Documents/uni/2/Bayes/Project/venv/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:1945: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.134\n",
      "The test error is: 0.135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(xtrain) #to standarize features to a zero mean and 1 variance!\n",
    "xtrain = scaler.transform(xtrain)\n",
    "xtest = scaler.transform(xtest)\n",
    "\n",
    "evaluate_clf(GaussianNB(), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "759b6c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/otis/Documents/uni/2/Bayes/Project/venv/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:1945: RuntimeWarning: divide by zero encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "/Users/otis/Documents/uni/2/Bayes/Project/venv/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:1945: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.866\n",
      "The test error is: 0.865\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(xtrain) #to standarize features to a zero mean and 1 variance!\n",
    "xtrain = scaler.transform(xtrain)\n",
    "xtest = scaler.transform(xtest)\n",
    "\n",
    "evaluate_clf(GaussianNB(), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585c9cc",
   "metadata": {},
   "source": [
    "### BBC Article Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f781bb",
   "metadata": {},
   "source": [
    "This dataset is a collection of BBC articles annotated with the type of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "71410b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Microsoft launches its own search\\n \\n Microso...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Warnings about junk mail deluge\\n \\n The amoun...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Microsoft gets the blogging bug\\n \\n Software ...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Gamers snap up new Sony PSP\\n \\n Gamers have b...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Apple laptop is 'greatest gadget'\\n \\n The App...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   news      type\n",
       "0     China had role in Yukos split-up\\n \\n China le...  business\n",
       "1     Oil rebounds from weather effect\\n \\n Oil pric...  business\n",
       "2     Indonesia 'declines debt freeze'\\n \\n Indonesi...  business\n",
       "3     $1m payoff for former Shell boss\\n \\n Shell is...  business\n",
       "4     US bank in $515m SEC settlement\\n \\n Five Bank...  business\n",
       "...                                                 ...       ...\n",
       "2220  Microsoft launches its own search\\n \\n Microso...      tech\n",
       "2221  Warnings about junk mail deluge\\n \\n The amoun...      tech\n",
       "2222  Microsoft gets the blogging bug\\n \\n Software ...      tech\n",
       "2223  Gamers snap up new Sony PSP\\n \\n Gamers have b...      tech\n",
       "2224  Apple laptop is 'greatest gadget'\\n \\n The App...      tech\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/articles.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8420c",
   "metadata": {},
   "source": [
    "We have the following five article types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6daacaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'entertainment', 'politics', 'sport', 'tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e83058",
   "metadata": {},
   "source": [
    "We transform the dataset in the same way as the previous dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "c08be7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(data[\"news\"])\n",
    "y = np.array(data[\"type\"])\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', max_features=1000) # count the amout of occurrences for each word\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "xtrain = cv.fit_transform(xtrain).toarray()\n",
    "xtest = cv.transform(xtest).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e2bb61",
   "metadata": {},
   "source": [
    "### Using Bernoulli Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8e6547d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.976\n",
      "The test error is: 0.937\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(BernoulliNB(alpha=1e-7), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940c2533",
   "metadata": {},
   "source": [
    "### Using Multinomial Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "37bb0f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.975\n",
      "The test error is: 0.951\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(MultinomialNB(alpha=2), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd4b6d",
   "metadata": {},
   "source": [
    "### Using Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "cc61f050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/otis/Documents/uni/2/Bayes/Project/venv/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:1945: RuntimeWarning: divide by zero encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "/Users/otis/Documents/uni/2/Bayes/Project/venv/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:1945: RuntimeWarning: invalid value encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.222\n",
      "The test error is: 0.258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(xtrain) #to standarize features to a zero mean and 1 variance!\n",
    "xtrain = scaler.transform(xtrain)\n",
    "xtest = scaler.transform(xtest)\n",
    "\n",
    "evaluate_clf(GaussianNB(), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cdfa5",
   "metadata": {},
   "source": [
    "## Raisin dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d0941",
   "metadata": {},
   "source": [
    "The raisin dataset was from a study of machine vision systems in order to distinguish between two distinct varieties of raisins, Kecimen and Besni, a total of 900 pices of raisin were obtained and distinct metric values were recorded and included in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d6adc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Raisin_Dataset.csv')\n",
    "x = np.array(df.drop(\"Class\", axis=1))\n",
    "y = np.array(df[\"Class\"])\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "eb249f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87524</td>\n",
       "      <td>442.246011</td>\n",
       "      <td>253.291155</td>\n",
       "      <td>0.819738</td>\n",
       "      <td>90546</td>\n",
       "      <td>0.758651</td>\n",
       "      <td>1184.040</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75166</td>\n",
       "      <td>406.690687</td>\n",
       "      <td>243.032436</td>\n",
       "      <td>0.801805</td>\n",
       "      <td>78789</td>\n",
       "      <td>0.684130</td>\n",
       "      <td>1121.786</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90856</td>\n",
       "      <td>442.267048</td>\n",
       "      <td>266.328318</td>\n",
       "      <td>0.798354</td>\n",
       "      <td>93717</td>\n",
       "      <td>0.637613</td>\n",
       "      <td>1208.575</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45928</td>\n",
       "      <td>286.540559</td>\n",
       "      <td>208.760042</td>\n",
       "      <td>0.684989</td>\n",
       "      <td>47336</td>\n",
       "      <td>0.699599</td>\n",
       "      <td>844.162</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79408</td>\n",
       "      <td>352.190770</td>\n",
       "      <td>290.827533</td>\n",
       "      <td>0.564011</td>\n",
       "      <td>81463</td>\n",
       "      <td>0.792772</td>\n",
       "      <td>1073.251</td>\n",
       "      <td>Kecimen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>83248</td>\n",
       "      <td>430.077308</td>\n",
       "      <td>247.838695</td>\n",
       "      <td>0.817263</td>\n",
       "      <td>85839</td>\n",
       "      <td>0.668793</td>\n",
       "      <td>1129.072</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>87350</td>\n",
       "      <td>440.735698</td>\n",
       "      <td>259.293149</td>\n",
       "      <td>0.808629</td>\n",
       "      <td>90899</td>\n",
       "      <td>0.636476</td>\n",
       "      <td>1214.252</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>99657</td>\n",
       "      <td>431.706981</td>\n",
       "      <td>298.837323</td>\n",
       "      <td>0.721684</td>\n",
       "      <td>106264</td>\n",
       "      <td>0.741099</td>\n",
       "      <td>1292.828</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>93523</td>\n",
       "      <td>476.344094</td>\n",
       "      <td>254.176054</td>\n",
       "      <td>0.845739</td>\n",
       "      <td>97653</td>\n",
       "      <td>0.658798</td>\n",
       "      <td>1258.548</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>85609</td>\n",
       "      <td>512.081774</td>\n",
       "      <td>215.271976</td>\n",
       "      <td>0.907345</td>\n",
       "      <td>89197</td>\n",
       "      <td>0.632020</td>\n",
       "      <td>1272.862</td>\n",
       "      <td>Besni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Area  MajorAxisLength  MinorAxisLength  Eccentricity  ConvexArea  \\\n",
       "0    87524       442.246011       253.291155      0.819738       90546   \n",
       "1    75166       406.690687       243.032436      0.801805       78789   \n",
       "2    90856       442.267048       266.328318      0.798354       93717   \n",
       "3    45928       286.540559       208.760042      0.684989       47336   \n",
       "4    79408       352.190770       290.827533      0.564011       81463   \n",
       "..     ...              ...              ...           ...         ...   \n",
       "895  83248       430.077308       247.838695      0.817263       85839   \n",
       "896  87350       440.735698       259.293149      0.808629       90899   \n",
       "897  99657       431.706981       298.837323      0.721684      106264   \n",
       "898  93523       476.344094       254.176054      0.845739       97653   \n",
       "899  85609       512.081774       215.271976      0.907345       89197   \n",
       "\n",
       "       Extent  Perimeter    Class  \n",
       "0    0.758651   1184.040  Kecimen  \n",
       "1    0.684130   1121.786  Kecimen  \n",
       "2    0.637613   1208.575  Kecimen  \n",
       "3    0.699599    844.162  Kecimen  \n",
       "4    0.792772   1073.251  Kecimen  \n",
       "..        ...        ...      ...  \n",
       "895  0.668793   1129.072    Besni  \n",
       "896  0.636476   1214.252    Besni  \n",
       "897  0.741099   1292.828    Besni  \n",
       "898  0.658798   1258.548    Besni  \n",
       "899  0.632020   1272.862    Besni  \n",
       "\n",
       "[900 rows x 8 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef4747",
   "metadata": {},
   "source": [
    "Naturally given that the data presented is continous, of the studied models we can only apply the Gaussian Naive Bayes. Let's observe the yielding results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e60193c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.838\n",
      "The test error is: 0.839\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(xtrain) #to standarize features to a zero mean and 1 variance!\n",
    "xtrain = scaler.transform(xtrain)\n",
    "xtest = scaler.transform(xtest)\n",
    "\n",
    "evaluate_clf(GaussianNB(), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df569f",
   "metadata": {},
   "source": [
    "Note that again, as in the Spam or Ham dataset, we have data with multiplicites, we expect the Multinomial Naive Bayes to perform the best, but we can actually perform tests in the 3 distinct models presented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d8066",
   "metadata": {},
   "source": [
    "## Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feab817",
   "metadata": {},
   "source": [
    "This dataset is similar to the Raisin dataset, this set of data containd 50 distinct samples of 3 distinct Iris plants, Iris setosa, Iris viginica and Iris Versicolor. 4  distinct features were recorded for each plant, we would want to be able to tell apart each of this type of Iris just with this given 4 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "281366fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>146</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>147</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>148</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>149</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "0      1            5.1           3.5            1.4           0.2   \n",
       "1      2            4.9           3.0            1.4           0.2   \n",
       "2      3            4.7           3.2            1.3           0.2   \n",
       "3      4            4.6           3.1            1.5           0.2   \n",
       "4      5            5.0           3.6            1.4           0.2   \n",
       "..   ...            ...           ...            ...           ...   \n",
       "145  146            6.7           3.0            5.2           2.3   \n",
       "146  147            6.3           2.5            5.0           1.9   \n",
       "147  148            6.5           3.0            5.2           2.0   \n",
       "148  149            6.2           3.4            5.4           2.3   \n",
       "149  150            5.9           3.0            5.1           1.8   \n",
       "\n",
       "            Species  \n",
       "0       Iris-setosa  \n",
       "1       Iris-setosa  \n",
       "2       Iris-setosa  \n",
       "3       Iris-setosa  \n",
       "4       Iris-setosa  \n",
       "..              ...  \n",
       "145  Iris-virginica  \n",
       "146  Iris-virginica  \n",
       "147  Iris-virginica  \n",
       "148  Iris-virginica  \n",
       "149  Iris-virginica  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/Iris.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414bca4",
   "metadata": {},
   "source": [
    "As in the Raisin Dataset, data is continous, and hence, the only course of action would be to approach it using a Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2936167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df.drop(\"Species\", axis=1))\n",
    "y = np.array(df[\"Species\"])\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bf3be975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.992\n",
      "The test error is: 1.000\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(xtrain) #to standarize features to a zero mean and 1 variance!\n",
    "xtrain = scaler.transform(xtrain)\n",
    "xtest = scaler.transform(xtest)\n",
    "\n",
    "evaluate_clf(GaussianNB(), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba12faa0",
   "metadata": {},
   "source": [
    "## Bayesian approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d6d36c",
   "metadata": {},
   "source": [
    "In this part we will take a Bayesian approach to the Naive Bayes classifier. That is, we define prior distributions over the parameters of the classifier. We define first analytical formulations of the Bayesian version of the Bernoulli and Multinomial models. Then we obtain the parameters from the data via probabilistic inference with Stan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e07d5",
   "metadata": {},
   "source": [
    "### Analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "7e729d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta, dirichlet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f73b9e",
   "metadata": {},
   "source": [
    "#### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "f9869587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesBernoulliNB(BernoulliNB):\n",
    "  def __init__(self, alpha=1, beta=1):\n",
    "    # set the parameters for the prior beta distribution\n",
    "    self.a = alpha,\n",
    "    self.b = beta\n",
    "\n",
    "  def compute_params(self, X):\n",
    "    X = X.clip(max=1)\n",
    "    return [beta.mean(self.a + X.sum(axis=0), self.b + len(X) - X.sum(axis=0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5767a8",
   "metadata": {},
   "source": [
    "#### Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8a77e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesMultinomialNB(MultinomialNB):\n",
    "  def __init__(self, alpha=[1]):\n",
    "    # set the parameters for the prior beta distribution\n",
    "    self.a = np.array(alpha)\n",
    "\n",
    "  def compute_params(self, X):\n",
    "    return [dirichlet.mean(self.a + X.sum(axis=0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af698dd",
   "metadata": {},
   "source": [
    "## Stan Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "65b782e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanNB(NaiveBayes):\n",
    "  def __init__(self, model_path, random_state=None):\n",
    "    self.seed = random_state\n",
    "    with open(model_path) as f:\n",
    "      self.stan_model = f.read()\n",
    "\n",
    "  def fit(self, X, y, data, param_names):\n",
    "    self.stan_data = data\n",
    "    self.param_names = param_names\n",
    "\n",
    "    posterior = stan.build(self.stan_model, data=self.stan_data, random_seed=self.seed)\n",
    "    self.stan_fit = posterior.sample(num_chains=4, num_samples=1000)\n",
    "\n",
    "    super().fit(X, y)\n",
    "    \n",
    "    self.class_probs = self.stan_fit['theta'].mean(axis=-1)\n",
    "    self.params = np.array(\n",
    "      [self.stan_fit[param].mean(axis=-1) for param in param_names]\n",
    "    )\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0485a6f",
   "metadata": {},
   "source": [
    "#### Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "609b7879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanBernoulliNB(StanNB):\n",
    "  def __init__(self, *, class_prior, param_prior, random_state=None):\n",
    "    self.alpha = class_prior\n",
    "    self.beta = param_prior\n",
    "    super().__init__(model_path='model_bernoulli.stan', random_state=random_state)\n",
    "\n",
    "  def compute_probabilities(self, X, phi):\n",
    "    return BernoulliNB.compute_probabilities(self, X, phi)\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    classes, y_int = np.unique(y, return_inverse=True)\n",
    "\n",
    "    data = {\n",
    "      'C': len(classes),    # num categories\n",
    "      'K': X.shape[1],      # num features\n",
    "      'N': X.shape[0],      # num samples\n",
    "      'y': y_int + 1,       # category of sample n\n",
    "      'x': X.clip(max=1),   # sample n\n",
    "      'alpha': self.alpha,  # class prior\n",
    "      'beta': self.beta     # feature prior\n",
    "    }\n",
    "\n",
    "    return super().fit(X, y, data, ['phi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2289ab3",
   "metadata": {},
   "source": [
    "#### Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "21c99a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanMultinomialNB(StanNB):\n",
    "  def __init__(self, *, class_prior, param_prior, random_state=None):\n",
    "    self.alpha = class_prior\n",
    "    self.beta = param_prior\n",
    "    super().__init__(model_path='model_multinomial.stan', random_state=random_state)\n",
    "\n",
    "  def compute_probabilities(self, X, phi):\n",
    "    return MultinomialNB.compute_probabilities(self, X, phi)\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    classes, y_int = np.unique(y, return_inverse=True)\n",
    "\n",
    "    idx = X.nonzero()\n",
    "    values = X[idx]\n",
    "    word_ids = np.repeat(idx[1],values)\n",
    "    doc_ids = np.repeat(idx[0],values)\n",
    "\n",
    "    data = {\n",
    "      'K': len(classes),    # num classes\n",
    "      'V': X.shape[1],      # num words\n",
    "      'M': X.shape[0],      # num docs\n",
    "      'N': len(word_ids),   # total word instances\n",
    "      'z': y_int + 1,       # class for doc m\n",
    "      'w': word_ids + 1,    # word n\n",
    "      'doc': doc_ids + 1,   # doc ID for word n\n",
    "      'alpha': self.alpha,  # class prior\n",
    "      'beta': self.beta     # feature prior\n",
    "    }\n",
    "\n",
    "    return super().fit(X, y, data, ['phi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750ecf5",
   "metadata": {},
   "source": [
    "#### Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "97b24cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanGaussianNB(StanNB):\n",
    "  def __init__(self, *, class_prior, mu_prior, sigma_prior, random_state=None):\n",
    "    self.alpha = class_prior\n",
    "    self.mu_mu, self.mu_sigma = mu_prior,\n",
    "    self.sigma_alpha, self.sigma_beta = sigma_prior\n",
    "    super().__init__(model_path='model_gaussian.stan', random_state=random_state)\n",
    "\n",
    "  def compute_probabilities(self, X, mean, sd):\n",
    "    return GaussianNB.compute_probabilities(self, X, mean, sd)\n",
    "  \n",
    "  def fit(self, X, y):\n",
    "    classes, y_int = np.unique(y, return_inverse=True)\n",
    "\n",
    "    data = {\n",
    "      'C': len(classes),                # num classes\n",
    "      'K': X.shape[1],                  # num features\n",
    "      'N': X.shape[0],                  # num samples\n",
    "      'y': y_int + 1,                   # class of sample n\n",
    "      'x': X,                           # sample n\n",
    "      'alpha': self.alpha,              # class prior\n",
    "      'mu_mu': self.mu_mu,              # feature mean prior\n",
    "      'mu_sigma': self.mu_sigma,        # feature mean prior\n",
    "      'sigma_alpha': self.sigma_alpha,  # feature sd prior\n",
    "      'sigma_beta': self.sigma_beta     # feature sd prior\n",
    "    }\n",
    "\n",
    "    return super().fit(X, y, data, ['mu', 'sigma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7539082",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c88e946",
   "metadata": {},
   "source": [
    "### Spam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "d4916533",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/spam.csv\", encoding='latin-1')\n",
    "x = np.array(data[\"message\"])\n",
    "y = np.array(data[\"class\"])\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', max_features=100)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, train_size=100, random_state=42, shuffle = True)\n",
    "\n",
    "xtrain = cv.fit_transform(xtrain).toarray()\n",
    "xtest = cv.transform(xtest).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "4690e045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.930\n",
      "The test error is: 0.878\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(BernoulliNB(alpha=0.1), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "779ca973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.980\n",
      "The test error is: 0.910\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(BayesBernoulliNB(alpha=0.1), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "f4e871e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter theta has no priors.\n",
      "Warning: The parameter phi has no priors.\n",
      "Sampling:   0%\n",
      "Sampling:   0% (1/8000)\n",
      "Sampling:   0% (2/8000)\n",
      "Sampling:   0% (3/8000)\n",
      "Sampling:   0% (4/8000)\n",
      "Sampling:   1% (103/8000)\n",
      "Sampling:   3% (202/8000)\n",
      "Sampling:   4% (301/8000)\n",
      "Sampling:   5% (400/8000)\n",
      "Sampling:   6% (500/8000)\n",
      "Sampling:   8% (600/8000)\n",
      "Sampling:   9% (700/8000)\n",
      "Sampling:  10% (800/8000)\n",
      "Sampling:  11% (900/8000)\n",
      "Sampling:  12% (1000/8000)\n",
      "Sampling:  14% (1100/8000)\n",
      "Sampling:  15% (1200/8000)\n",
      "Sampling:  16% (1300/8000)\n",
      "Sampling:  18% (1400/8000)\n",
      "Sampling:  19% (1500/8000)\n",
      "Sampling:  20% (1600/8000)\n",
      "Sampling:  21% (1700/8000)\n",
      "Sampling:  22% (1800/8000)\n",
      "Sampling:  24% (1900/8000)\n",
      "Sampling:  25% (2000/8000)\n",
      "Sampling:  26% (2100/8000)\n",
      "Sampling:  28% (2200/8000)\n",
      "Sampling:  29% (2300/8000)\n",
      "Sampling:  30% (2400/8000)\n",
      "Sampling:  31% (2500/8000)\n",
      "Sampling:  32% (2600/8000)\n",
      "Sampling:  34% (2700/8000)\n",
      "Sampling:  35% (2800/8000)\n",
      "Sampling:  36% (2900/8000)\n",
      "Sampling:  38% (3000/8000)\n",
      "Sampling:  39% (3100/8000)\n",
      "Sampling:  40% (3200/8000)\n",
      "Sampling:  41% (3300/8000)\n",
      "Sampling:  42% (3400/8000)\n",
      "Sampling:  44% (3500/8000)\n",
      "Sampling:  45% (3600/8000)\n",
      "Sampling:  46% (3701/8000)\n",
      "Sampling:  48% (3801/8000)\n",
      "Sampling:  48% (3802/8000)\n",
      "Sampling:  49% (3903/8000)\n",
      "Sampling:  50% (4004/8000)\n",
      "Sampling:  51% (4103/8000)\n",
      "Sampling:  53% (4202/8000)\n",
      "Sampling:  54% (4301/8000)\n",
      "Sampling:  55% (4400/8000)\n",
      "Sampling:  56% (4500/8000)\n",
      "Sampling:  58% (4600/8000)\n",
      "Sampling:  59% (4700/8000)\n",
      "Sampling:  60% (4800/8000)\n",
      "Sampling:  61% (4900/8000)\n",
      "Sampling:  62% (5000/8000)\n",
      "Sampling:  64% (5100/8000)\n",
      "Sampling:  65% (5200/8000)\n",
      "Sampling:  66% (5300/8000)\n",
      "Sampling:  68% (5400/8000)\n",
      "Sampling:  69% (5500/8000)\n",
      "Sampling:  70% (5600/8000)\n",
      "Sampling:  71% (5700/8000)\n",
      "Sampling:  72% (5800/8000)\n",
      "Sampling:  74% (5900/8000)\n",
      "Sampling:  75% (6000/8000)\n",
      "Sampling:  76% (6100/8000)\n",
      "Sampling:  78% (6200/8000)\n",
      "Sampling:  79% (6300/8000)\n",
      "Sampling:  80% (6400/8000)\n",
      "Sampling:  81% (6500/8000)\n",
      "Sampling:  82% (6600/8000)\n",
      "Sampling:  84% (6700/8000)\n",
      "Sampling:  85% (6800/8000)\n",
      "Sampling:  86% (6900/8000)\n",
      "Sampling:  88% (7000/8000)\n",
      "Sampling:  89% (7100/8000)\n",
      "Sampling:  90% (7200/8000)\n",
      "Sampling:  91% (7300/8000)\n",
      "Sampling:  92% (7400/8000)\n",
      "Sampling:  94% (7500/8000)\n",
      "Sampling:  95% (7600/8000)\n",
      "Sampling:  96% (7700/8000)\n",
      "Sampling:  98% (7800/8000)\n",
      "Sampling:  99% (7900/8000)\n",
      "Sampling: 100% (8000/8000)\n",
      "Sampling: 100% (8000/8000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.009817 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 98.17 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.009711 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 97.11 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.003188 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 31.88 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.005479 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 54.79 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.970\n",
      "The test error is: 0.906\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(\n",
    "  StanBernoulliNB(\n",
    "    class_prior=[0.1,0.1],\n",
    "    param_prior=np.full((xtrain.shape[1], 2), 0.1),\n",
    "    random_state=42),\n",
    "  xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "a162d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.980\n",
      "The test error is: 0.912\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(MultinomialNB(alpha=0.1), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "0240171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.980\n",
      "The test error is: 0.912\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(BayesMultinomialNB(alpha=[0.1]), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "b7e9f1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter theta has no priors.\n",
      "Warning: The parameter phi has no priors.\n",
      "Sampling:   0%\n",
      "Sampling:   0% (1/8000)\n",
      "Sampling:   0% (2/8000)\n",
      "Sampling:   0% (3/8000)\n",
      "Sampling:   0% (4/8000)\n",
      "Sampling:   1% (103/8000)\n",
      "Sampling:   3% (202/8000)\n",
      "Sampling:   4% (301/8000)\n",
      "Sampling:   5% (400/8000)\n",
      "Sampling:   6% (500/8000)\n",
      "Sampling:   8% (600/8000)\n",
      "Sampling:   9% (700/8000)\n",
      "Sampling:  10% (800/8000)\n",
      "Sampling:  11% (900/8000)\n",
      "Sampling:  12% (1000/8000)\n",
      "Sampling:  14% (1100/8000)\n",
      "Sampling:  15% (1200/8000)\n",
      "Sampling:  16% (1300/8000)\n",
      "Sampling:  18% (1400/8000)\n",
      "Sampling:  19% (1500/8000)\n",
      "Sampling:  20% (1600/8000)\n",
      "Sampling:  21% (1700/8000)\n",
      "Sampling:  22% (1800/8000)\n",
      "Sampling:  24% (1900/8000)\n",
      "Sampling:  25% (2000/8000)\n",
      "Sampling:  26% (2100/8000)\n",
      "Sampling:  28% (2200/8000)\n",
      "Sampling:  29% (2300/8000)\n",
      "Sampling:  30% (2400/8000)\n",
      "Sampling:  31% (2500/8000)\n",
      "Sampling:  32% (2600/8000)\n",
      "Sampling:  34% (2700/8000)\n",
      "Sampling:  35% (2800/8000)\n",
      "Sampling:  36% (2900/8000)\n",
      "Sampling:  38% (3000/8000)\n",
      "Sampling:  39% (3100/8000)\n",
      "Sampling:  40% (3200/8000)\n",
      "Sampling:  41% (3300/8000)\n",
      "Sampling:  42% (3400/8000)\n",
      "Sampling:  44% (3500/8000)\n",
      "Sampling:  45% (3601/8000)\n",
      "Sampling:  46% (3701/8000)\n",
      "Sampling:  48% (3801/8000)\n",
      "Sampling:  48% (3802/8000)\n",
      "Sampling:  49% (3903/8000)\n",
      "Sampling:  50% (4004/8000)\n",
      "Sampling:  51% (4103/8000)\n",
      "Sampling:  53% (4202/8000)\n",
      "Sampling:  54% (4301/8000)\n",
      "Sampling:  55% (4400/8000)\n",
      "Sampling:  56% (4500/8000)\n",
      "Sampling:  58% (4600/8000)\n",
      "Sampling:  59% (4700/8000)\n",
      "Sampling:  60% (4800/8000)\n",
      "Sampling:  61% (4900/8000)\n",
      "Sampling:  62% (5000/8000)\n",
      "Sampling:  64% (5100/8000)\n",
      "Sampling:  65% (5200/8000)\n",
      "Sampling:  66% (5300/8000)\n",
      "Sampling:  68% (5400/8000)\n",
      "Sampling:  69% (5500/8000)\n",
      "Sampling:  70% (5600/8000)\n",
      "Sampling:  71% (5700/8000)\n",
      "Sampling:  72% (5800/8000)\n",
      "Sampling:  74% (5900/8000)\n",
      "Sampling:  75% (6000/8000)\n",
      "Sampling:  76% (6100/8000)\n",
      "Sampling:  78% (6200/8000)\n",
      "Sampling:  79% (6300/8000)\n",
      "Sampling:  80% (6400/8000)\n",
      "Sampling:  81% (6500/8000)\n",
      "Sampling:  82% (6600/8000)\n",
      "Sampling:  84% (6700/8000)\n",
      "Sampling:  85% (6800/8000)\n",
      "Sampling:  86% (6900/8000)\n",
      "Sampling:  88% (7000/8000)\n",
      "Sampling:  89% (7100/8000)\n",
      "Sampling:  90% (7200/8000)\n",
      "Sampling:  91% (7300/8000)\n",
      "Sampling:  92% (7400/8000)\n",
      "Sampling:  94% (7500/8000)\n",
      "Sampling:  95% (7600/8000)\n",
      "Sampling:  96% (7700/8000)\n",
      "Sampling:  98% (7800/8000)\n",
      "Sampling:  99% (7900/8000)\n",
      "Sampling: 100% (8000/8000)\n",
      "Sampling: 100% (8000/8000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.000186 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 1.86 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.000184 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 1.84 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.000388 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 3.88 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.001408 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 14.08 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.950\n",
      "The test error is: 0.873\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(\n",
    "  StanMultinomialNB(\n",
    "    class_prior=[0.1, 0.1],\n",
    "    param_prior=np.full(xtrain.shape[1], 0.1),\n",
    "    random_state=42),\n",
    "  xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d3da3",
   "metadata": {},
   "source": [
    "### Article dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "d4916533",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/articles.csv\")\n",
    "x = np.array(data[\"news\"])\n",
    "y = np.array(data[\"type\"])\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', max_features=100)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, train_size=100, random_state=42, shuffle=True)\n",
    "\n",
    "xtrain = cv.fit_transform(xtrain).toarray()\n",
    "xtest = cv.transform(xtest).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "093d2d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "4690e045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.940\n",
      "The test error is: 0.816\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(BernoulliNB(alpha=0.01), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "779ca973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.960\n",
      "The test error is: 0.818\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(BayesBernoulliNB(alpha=0.1), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "f4e871e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter theta has no priors.\n",
      "Warning: The parameter phi has no priors.\n",
      "Sampling:   0%\n",
      "Sampling:   0% (1/8000)\n",
      "Sampling:   0% (2/8000)\n",
      "Sampling:   0% (3/8000)\n",
      "Sampling:   0% (4/8000)\n",
      "Sampling:   1% (103/8000)\n",
      "Sampling:   3% (202/8000)\n",
      "Sampling:   4% (301/8000)\n",
      "Sampling:   5% (400/8000)\n",
      "Sampling:   6% (500/8000)\n",
      "Sampling:   8% (600/8000)\n",
      "Sampling:   9% (700/8000)\n",
      "Sampling:  10% (800/8000)\n",
      "Sampling:  11% (900/8000)\n",
      "Sampling:  12% (1000/8000)\n",
      "Sampling:  14% (1100/8000)\n",
      "Sampling:  15% (1200/8000)"
     ]
    }
   ],
   "source": [
    "evaluate_clf(\n",
    "  StanBernoulliNB(\n",
    "    class_prior=np.full(num_classes, 0.1),\n",
    "    param_prior=np.full((xtrain.shape[1], 2), 0.1),\n",
    "    random_state=42),\n",
    "  xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.980\n",
      "The test error is: 0.912\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(MultinomialNB(alpha=0.1), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.980\n",
      "The test error is: 0.912\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(BayesMultinomialNB(alpha=[0.1]), xtrain, xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9f1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter theta has no priors.\n",
      "Warning: The parameter phi has no priors.\n",
      "Sampling:   0%\n",
      "Sampling: 100%, done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.003971 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 39.71 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.001222 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 12.22 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.000341 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 3.41 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.001379 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 13.79 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train error is: 0.930\n",
      "The test error is: 0.873\n"
     ]
    }
   ],
   "source": [
    "evaluate_clf(\n",
    "  StanMultinomialNB(\n",
    "    class_prior=[1,1],\n",
    "    param_prior=np.full(xtrain.shape[1], 1),\n",
    "    random_state=42),\n",
    "  xtrain, xtest, ytrain, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "686a0a42d112c097b6cb202e9e1b570eab92f482312aa151e86604e172b50afa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
